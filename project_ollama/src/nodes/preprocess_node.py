# preprocess_data.py
# 
# This file extracts columns and summaries of each file extension and outputs to a json file: src/data/*.json
# The reason to do this is so that each file type (e.g. haloproperties, galaxyparticles) has its columns easily read by LLM
# and enable autogenerated descriptions of variables.

import os
import sys
import json
import glob
from typing import Dict, Any

genericio_path = "genericio/legacy_python/"
sys.path.append(genericio_path)

import genericio as gio
from src.llm.llm_client import llm
from src.langgraph_class.states import AnalysisState

file_patterns = ['m000p-*.accumulatedcores',
 'm000p-*.bighaloparticles',
 'm000p-*.coreparticles',
 'm000p-*.galaxyparticles',
 'm000p-*.galaxyparticles.subgrid',
 'm000p-*.galaxyproperties',
 'm000p-*.galaxypropertybins',
 'm000p-*.haloparticles',
 'm000p-*.haloparticletags',
 'm000p-*.haloproperties',
 'm000p-*.sodbighaloparticles',
 'm000p-*.sodbighaloparticles.subgrid']

OUTPUT_DIR = "src/data/JSON/"


def preprocess_node(state: AnalysisState) -> AnalysisState:
    base_dir = state.get("base_dir", "")
    if base_dir == "":
        raise ValueError("Base data directory is empty.")
    output_dir = OUTPUT_DIR
    
    """preprocess_data() pipeline used in main.py to run all preprocessing steps for generating necessary JSON files"""
    os.makedirs(output_dir, exist_ok = True)
    os.makedirs(f"{output_dir}descriptions/", exist_ok = True)

    # setup_json sets up data_variables.json file for adding descriptions
    if not os.path.exists(f"{output_dir}data_variables.json") or os.path.getsize(f"{output_dir}data_variables.json")==0:
        print("Writing initial data_variables.json...")
        setup_json(base_dir, False)
    else:
        print("Initial data_variables.json already exists and is non-empty. Skipping setup_json(base_dir, False).")
        return state

    # uses data_variables.json to send variables to LLM for description
    if not glob.glob(f"{output_dir}descriptions/*.json"):
        print("Generating descriptions JSON files...")
        autogenerate_json_description()
    else:
        print("Descriptions JSON files already exist. Skipping autogenerate_json_description().")

    # data_variables.json updated with all descriptions
    # Only run second setup_json if the first one ran
    print("Updating data_variables.json with descriptions...")
    setup_json(base_dir, True)


    with open(f"{output_dir}data_variables.json", "r") as f:
        data_variables = json.load(f)

    # state["data_variables"] = data_variables
    state["preprocessing_complete"] = True
    return state


def setup_json(base_dir: str, descriptions_exist: bool):
    """
    This function finds all files matching one of the HACC data extensions within a folder 
    and then creates a .json file compiling all extensions and their parameters

    base_dir: str - base_dir is set to flamingo_B_1_analysis path

    descriptions_exist: bool - False means that autogenerate_json_description() has not been run yet. 
        True means it has been run and this function can update the json with variable descriptions
    """
    if not os.path.isdir(base_dir):
        raise ValueError("The specified path is not a valid directory.")

    if not glob.glob(base_dir):
        raise ValueError("No files matching patterns found in base_dir")

    output_path = os.path.join(OUTPUT_DIR,"data_variables.json")

    # Group files by extension
    extension_map = {}
    for pattern in file_patterns:
        for file_path in glob.glob(os.path.join(base_dir, pattern)):
            extension = ".".join(os.path.basename(file_path).split(".")[-1:])
            if extension == "subgrid":
                extension = ".".join(os.path.basename(file_path).split(".")[-2:])
            extension_map.setdefault(extension,[]).append(file_path)


    with open("src/data/file_descriptions.json", "r") as f:
        file_descriptions = json.load(f)

    # Create json for variables from all extension patterns
    json_data = {}
    for extension, files in extension_map.items():
        sample_file = files[0]
        col_names = gio.get_scalars(sample_file)[1]
        
        json_data[extension] = {
            "description": file_descriptions[extension], 
            "columns":{
                col: {
                    "type":"float",
                    "description":"",
                    "nullable": "False"
                }
                for col in col_names
            }
        }

        if descriptions_exist:
            # Load the description file if it exists
            description_file_json = f"{OUTPUT_DIR}/descriptions/{extension}_descriptions.json"

            descriptions = {}
            
            with open(description_file_json, "r", encoding="utf-8") as f:
                descriptions = json.load(f)

            # Merge descriptions into json_data
            for var, desc in descriptions.items():
                if var in json_data[extension]["columns"]:
                    json_data[extension]["columns"][var]["description"] = desc
    
    # Write json to src/JSON
    with open(output_path, "w") as f:
        json.dump(json_data, f, indent = 2)
    print(f"[OK] Wrote schema for data variables to {output_path}")


def helper_describe_json_vars():
    """ Helper function gets all files with a file_pattern extension and outputs a dictionary of extension: list of variables"""
    with open(f"{OUTPUT_DIR}data_variables.json", "r", encoding="utf-8") as f:
        data = json.load(f)
    
    # Get all file extensiones and variables for each extension
    all_vars = []
    all_extension = []
    all_description = []
    for extension, data in data.items():
        all_extension.append(extension)
        all_description.append(data["description"])
        vars = list(data["columns"].keys())
        all_vars.append(vars)

    return all_extension, all_description, all_vars


def autogenerate_json_description():
    """
    This function gets all extensions and variables from helper_describe_json_vars() 
    and asks LLM to describe each variable and output description to: {extension}_descriptions.txt.

    This output can then be read back in from setup_json() to update data_variables.json with variable descriptions.
    """

    all_extension, all_description, all_vars = helper_describe_json_vars()

    def chunk_list(lst, size):
        """Yield successive size-sized chunks from list"""
        for i in range(0, len(lst), size):
            yield lst[i:i + size]
    
    for extension, description, vars in zip(all_extension, all_description, all_vars):
        # # If list of variables is too long, LLM stalls. Need to break list into chunks before feeding into LLM.
        # with open(f"{OUTPUT_DIR}descriptions/{extension}_descriptions.txt", "w", encoding="utf-8") as file:
        #     pass  # This creates/truncates the file
        descriptions = {}

        # Chunking for long variable lists
        chunk_size = 20
        chunks = list(chunk_list(vars, chunk_size)) if len(vars) > chunk_size else [vars]

        for i, chunk in enumerate(chunks):
            prompt = f"""You are an expert cosmology data scientist using HACC data. 
                This is a table describing '{extension}'. {description}, return a JSON object with clear descriptions for each variable based on the context of {extension}. Include {extension} as part of the description when relevant.
                
                Return only a valid JSON object, formatted as:
                {{
                "variable1": "description1",
                "variable2": "description2",
                ...
                }}
                Do not wrap it in triple quotes or code blocks.

                Variables:
                {chunk}
                """

            print(f"Asking LLM to describe {extension} - chunk {i + 1}/{len(chunks)}")
            response = llm.invoke(prompt)

            try:
                chunk_dict = json.loads(response.content)
                if not isinstance(chunk_dict, dict):
                    raise ValueError("Expected a JSON object.")
                descriptions.update(chunk_dict)
            except json.JSONDecodeError:
                print(f"⚠️ Failed to decode JSON for chunk {i + 1} of {extension}")
                print("Raw response:\n", response.content)

        # Write the final combined result to a single .json file
        output_path = f"{OUTPUT_DIR}descriptions/{extension}_descriptions.json"
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(descriptions, f, indent=2)
            print(f"✅ Descriptions for {extension} written to {output_path}")